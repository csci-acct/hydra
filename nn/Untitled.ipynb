{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import torch\n",
    "import utilities\n",
    "import gc\n",
    "\n",
    "from torchtext.experimental.datasets import WikiText103\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from Container import NNContainer\n",
    "\n",
    "from customLayers import BertEmbedding, BertTransformerEncoderLayer\n",
    "\n",
    "from torch.nn import Linear\n",
    "\n",
    "import torch\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "\n",
    "class MultiheadAttentionContainer(torch.nn.Module):\n",
    "    def __init__(self, nhead, in_proj_container, attention_layer, out_proj, batch_first=False):\n",
    "\n",
    "        super(MultiheadAttentionContainer, self).__init__()\n",
    "        self.nhead = nhead\n",
    "        self.in_proj_container = in_proj_container\n",
    "        self.attention_layer = attention_layer\n",
    "        self.out_proj = out_proj\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
    "                attn_mask: Optional[torch.Tensor] = None,\n",
    "                bias_k: Optional[torch.Tensor] = None,\n",
    "                bias_v: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        if self.batch_first:\n",
    "            query, key, value = query.transpose(-3, -2), key.transpose(-3, -2), value.transpose(-3, -2)\n",
    "\n",
    "        \n",
    "        tgt_len, src_len, bsz, embed_dim = query.size(-3), key.size(-3), query.size(-2), query.size(-1)\n",
    "        \n",
    "        q, k, v = self.in_proj_container(query, key, value)\n",
    "        return q, k\n",
    "        \n",
    "        \n",
    "        assert q.size(-1) % self.nhead == 0, \"query's embed_dim must be divisible by the number of heads\"\n",
    "        head_dim = q.size(-1) // self.nhead\n",
    "        q = q.reshape(tgt_len, bsz * self.nhead, head_dim)\n",
    "\n",
    "        assert k.size(-1) % self.nhead == 0, \"key's embed_dim must be divisible by the number of heads\"\n",
    "        head_dim = k.size(-1) // self.nhead\n",
    "        k = k.reshape(src_len, bsz * self.nhead, head_dim)\n",
    "\n",
    "        assert v.size(-1) % self.nhead == 0, \"value's embed_dim must be divisible by the number of heads\"\n",
    "        head_dim = v.size(-1) // self.nhead\n",
    "        v = v.reshape(src_len, bsz * self.nhead, head_dim)\n",
    "\n",
    "        \n",
    "        \n",
    "        attn_output, attn_output_weights = self.attention_layer(q, k, v, attn_mask=attn_mask,\n",
    "                                                                bias_k=bias_k, bias_v=bias_v)\n",
    "        attn_output = attn_output.reshape(tgt_len, bsz, embed_dim)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        if self.batch_first:\n",
    "            attn_output = attn_output.transpose(-3, -2)\n",
    "\n",
    "        return attn_output, attn_output_weights\n",
    "\n",
    "\n",
    "class ScaledDotProduct(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.0, batch_first=False):\n",
    "        super(ScaledDotProduct, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
    "                attn_mask: Optional[torch.Tensor] = None,\n",
    "                bias_k: Optional[torch.Tensor] = None,\n",
    "                bias_v: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if self.batch_first:\n",
    "            query, key, value = query.transpose(-3, -2), key.transpose(-3, -2), value.transpose(-3, -2)\n",
    "\n",
    "        if bias_k is not None and bias_v is not None:\n",
    "            assert key.size(-1) == bias_k.size(-1) and key.size(-2) == bias_k.size(-2) and bias_k.size(-3) == 1, \\\n",
    "                \"Shape of bias_k is not supported\"\n",
    "            assert value.size(-1) == bias_v.size(-1) and value.size(-2) == bias_v.size(-2) and bias_v.size(-3) == 1, \\\n",
    "                \"Shape of bias_v is not supported\"\n",
    "            key = torch.cat([key, bias_k])\n",
    "            value = torch.cat([value, bias_v])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = torch.nn.functional.pad(attn_mask, (0, 1))\n",
    "\n",
    "        tgt_len, head_dim = query.size(-3), query.size(-1)\n",
    "        assert query.size(-1) == key.size(-1) == value.size(-1), \"The feature dim of query, key, value must be equal.\"\n",
    "        assert key.size() == value.size(), \"Shape of key, value must match\"\n",
    "        src_len = key.size(-3)\n",
    "        batch_heads = max(query.size(-2), key.size(-2))\n",
    "\n",
    "        # Scale query\n",
    "        query, key, value = query.transpose(-2, -3), key.transpose(-2, -3), value.transpose(-2, -3)\n",
    "        query = query * (float(head_dim) ** -0.5)\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dim() != 3:\n",
    "                raise RuntimeError('attn_mask must be a 3D tensor.')\n",
    "            if (attn_mask.size(-1) != src_len) or (attn_mask.size(-2) != tgt_len) or \\\n",
    "               (attn_mask.size(-3) != 1 and attn_mask.size(-3) != batch_heads):\n",
    "                raise RuntimeError('The size of the attn_mask is not correct.')\n",
    "            if attn_mask.dtype != torch.bool:\n",
    "                raise RuntimeError('Only bool tensor is supported for attn_mask')\n",
    "\n",
    "        # Dot product of q, k\n",
    "        attn_output_weights = torch.matmul(query, key.transpose(-2, -1))\n",
    "        if attn_mask is not None:\n",
    "            attn_output_weights.masked_fill_(attn_mask, -1e8,)\n",
    "        attn_output_weights = torch.nn.functional.softmax(attn_output_weights, dim=-1)\n",
    "        attn_output_weights = torch.nn.functional.dropout(attn_output_weights, p=self.dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_output_weights, value)\n",
    "\n",
    "        if self.batch_first:\n",
    "            return attn_output, attn_output_weights\n",
    "        else:\n",
    "            return attn_output.transpose(-3, -2), attn_output_weights\n",
    "\n",
    "\n",
    "class InProjContainer(torch.nn.Module):\n",
    "    def __init__(self, query_proj, key_proj, value_proj):\n",
    "        super(InProjContainer, self).__init__()\n",
    "        self.query_proj = query_proj\n",
    "        self.key_proj = key_proj\n",
    "        self.value_proj = value_proj\n",
    "\n",
    "    def forward(self,\n",
    "                query: torch.Tensor,\n",
    "                key: torch.Tensor,\n",
    "                value: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        return self.query_proj(query), self.key_proj(key), self.value_proj(value)\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(nbatch, sz):\n",
    "    r\"\"\"Generate a square mask for the sequence. The masked positions are filled with True.\n",
    "        Unmasked positions are filled with False.\n",
    "    Args:\n",
    "        nbatch: the number of batch size\n",
    "        sz: the size of square mask\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1).repeat(nbatch, 1, 1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "print(\"\\nPreparing to load dataset....\")\n",
    "vocab = torch.load(\"../../vocab/torchtext_bert_vocab.pt\")\n",
    "\n",
    "def process_raw_data(whole_data, frac_ns):\n",
    "    processed_data = []\n",
    "    for _idx in range(len(whole_data)):\n",
    "        if (_idx % 10000 == 0):\n",
    "            print(\"Processing data....{}\".format(_idx), end='\\r', flush=True)\n",
    "        item = whole_data[_idx]\n",
    "    \n",
    "        if isinstance(item, list):\n",
    "            item = torch.tensor(item)\n",
    "\n",
    "        if (len(item) > 1):\n",
    "            # idx to split the text into two sentencd\n",
    "            split_idx = torch.randint(1, len(item), size=(1, 1)).item()\n",
    "            # Index 2 means same sentence label. Initial true int(1)\n",
    "            processed_data.append([item[:split_idx], item[split_idx:], 1])\n",
    "    # Random shuffle data to have args.frac_ns next sentence set up\n",
    "    shuffle_idx1 = torch.randperm(len(processed_data))\n",
    "    shuffle_idx2 = torch.randperm(len(processed_data))\n",
    "    num_shuffle = int(len(processed_data) * frac_ns)\n",
    "    shuffle_zip = list(zip(shuffle_idx1, shuffle_idx2))[:num_shuffle]\n",
    "    for (i, j) in shuffle_zip:\n",
    "        processed_data[i][1] = processed_data[j][0]\n",
    "        processed_data[i][2] = int(0)  # Switch same sentence label to false 0\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "def collate_batch(batch, bptt, cls_id, sep_id, pad_id):\n",
    "    # Fix sequence length to args.bptt with padding or trim\n",
    "    seq_list = []\n",
    "    tok_type = []\n",
    "    same_sentence_labels = []\n",
    "    for item in batch:\n",
    "        qa_item = torch.cat([item[0], torch.tensor([sep_id]).long(), item[1], torch.tensor([sep_id]).long()])\n",
    "        if qa_item.size(0) > bptt:\n",
    "            qa_item = qa_item[:bptt]\n",
    "        elif qa_item.size(0) < bptt:\n",
    "            qa_item = torch.cat((qa_item,\n",
    "                                 torch.tensor([pad_id] * (bptt -\n",
    "                                                          qa_item.size(0)))))\n",
    "        seq_list.append(qa_item)\n",
    "        _tok_tp = torch.ones((qa_item.size(0)))\n",
    "        _idx = min(len(item[0]) + 1, bptt)\n",
    "        _tok_tp[:_idx] = 0.0\n",
    "        tok_type.append(_tok_tp)\n",
    "        same_sentence_labels.append(item[2])\n",
    "    seq_input = torch.stack(seq_list).long().t().contiguous()\n",
    "    seq_input = torch.cat((torch.tensor([[cls_id] * seq_input.size(1)]).long(), seq_input))\n",
    "    seq_input = seq_input.transpose(0, 1)\n",
    "    tok_type = torch.stack(tok_type).long().t().contiguous()\n",
    "    tok_type = torch.cat((torch.tensor([[0] * tok_type.size(1)]).long(), tok_type))\n",
    "    return seq_input, tok_type, torch.tensor(same_sentence_labels).long().contiguous()\n",
    "\n",
    "\n",
    "\n",
    "dataset = WikiText103(vocab=vocab, split='valid') # set to train for real testing.pi\n",
    "dataset = process_raw_data(dataset, frac_ns=0.5)\n",
    "cls_id = vocab.stoi['<cls>']\n",
    "pad_id = vocab.stoi['<pad>']\n",
    "sep_id = vocab.stoi['<sep>']\n",
    "bptt = 128\n",
    "end = timer()\n",
    "\n",
    "print(\"Dataset Loaded in {} seconds.\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_1 = BertEmbedding(len(vocab), 768).to(\"cuda:0\")\n",
    "\n",
    "module_2 = BertTransformerEncoderLayer(768, 16, 1024, 0.5).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True,\n",
    "                            collate_fn=lambda b: collate_batch(b, bptt, cls_id, sep_id, pad_id))\n",
    "\n",
    "batch = next(iter(dataloader))[0:-1]\n",
    "\n",
    "batch = [ x.to(\"cuda:0\") for x in batch ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def get_free_space(idx=0):\n",
    "    nvmlInit()\n",
    "    h = nvmlDeviceGetHandleByIndex(idx)\n",
    "    info = nvmlDeviceGetMemoryInfo(h)\n",
    "    return info.free\n",
    "\n",
    "\n",
    "linear_layer = torch.nn.Linear(768, 768).to(\"cuda:0\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(get_free_space(0))\n",
    "    a_detach = torch.zeros((128, 129, 768)).to(\"cuda:0\")\n",
    "    d = linear_layer(a_detach)\n",
    "    del d\n",
    "    del a_detach\n",
    "    del linear_layer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(get_free_space(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-beads",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
